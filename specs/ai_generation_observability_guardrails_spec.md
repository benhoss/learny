# AI Generation Observability & Guardrails Specification (v1)

## 1. Purpose

Define the first production-ready specification for storing, monitoring, and governing all AI-generated outputs across Learny so we can:

- Prevent compliance and safety incidents before content reaches children.
- Detect problematic generations quickly when they occur.
- Track quality and operational progress over time.
- Build a high-quality dataset for model and prompt improvement.

This spec covers text generation, structured JSON generation, and future multimodal outputs generated by backend AI services.

---

## 2. Goals and Non-Goals

### 2.1 Goals

- Capture an auditable trail for every AI generation request and response.
- Introduce enforceable guardrails (policy checks + schema checks + risk scoring).
- Enable human review workflows for flagged outputs.
- Provide near real-time operational and safety monitoring.
- Retain analyzable historical data for experimentation and continuous improvement.

### 2.2 Non-Goals (v1)

- Re-training custom foundation models.
- Building a full legal records management platform.
- Building child-facing explanations for moderation decisions.

---

## 3. Key Risks Addressed

- Harmful, age-inappropriate, or unsafe content exposure.
- Biased or discriminatory outputs.
- Hallucinated facts in learning content.
- Leakage of personal data into prompts, outputs, logs, or analytics.
- Missing traceability during incidents ("we cannot reconstruct what happened").
- Inability to measure quality trends or intervention effectiveness.

---

## 4. Scope

### 4.1 In Scope

- Backend generation pipeline events from prompt creation to final persisted output.
- Safety checks before and after model completion.
- Policy versioning and explainable decision logs.
- Review queue for flagged generations.
- Observability dashboards + alerting.
- Data retention + deletion handling for compliance.

### 4.2 Out of Scope

- UI redesign for parent/child apps (except data dependencies).
- Third-party SIEM integration (can be phase 2).

---

## 5. Definitions

- **Generation Run**: One end-to-end AI invocation tied to a product action.
- **Artifact**: Any generated output (JSON pack, quiz items, explanation text, OCR extract, etc.).
- **Guardrail Check**: Deterministic or model-based validation step returning pass/warn/fail.
- **Risk Score**: Aggregated score representing policy/compliance risk.
- **Review Case**: Human moderation task created from a flagged run.

---

## 6. Functional Requirements

### FR-1: Full Generation Traceability

For each generation run, store:

- Request metadata (actor type, child/profile/document IDs, feature name, correlation ID).
- Model metadata (provider, model version, temperature, max tokens, region).
- Input snapshots (prompt template version + prompt variables).
- Output snapshots (raw output + normalized/parsing result).
- Guardrail decisions and reasons.
- Final serving decision (served, blocked, served-with-redaction, sent-to-review).

### FR-2: Guardrail Enforcement Pipeline

Every generation must pass through ordered checks:

1. Input PII policy check.
2. Prompt policy check (for banned intents / unsafe instructions).
3. Output schema validation.
4. Output safety moderation.
5. Age-appropriateness policy check.
6. Academic integrity check (anti-cheating signals).

If any check returns fail, content is blocked from user delivery and routed per policy.

### FR-3: Policy Versioning

- Every check uses a versioned policy/rule set.
- Decision logs must record `policy_name`, `policy_version`, and `rule_id` matches.
- Historical data must remain interpretable when policies evolve.

### FR-4: Human-in-the-Loop Review

- Create review cases for configurable risk tiers.
- Moderator can mark: approved, rejected, redacted, escalate-legal.
- All moderator actions are audited with actor ID and timestamp.

### FR-5: Monitoring & Alerts

- Dashboard metrics for volume, latency, pass/fail rates, and incident trends.
- Alerts for sudden spikes in safety failures, schema failures, or provider drift.
- SLA monitors on end-to-end generation latency and queue backlogs.

### FR-6: Improvement Dataset Export

- Support filtered export of sanitized generation data for analysis.
- Exports must exclude direct identifiers by default.
- Keep linkage keys needed for offline quality scoring experiments.

---

## 7. Non-Functional Requirements

- **Availability**: Logging path should not lose data during transient failures (at-least-once event persistence).
- **Latency budget**: Guardrail overhead target ≤ 400 ms p95 for synchronous user flows.
- **Scalability**: Handle 10x current generation volume without redesign.
- **Security**: Encryption in transit and at rest for all generation logs.
- **Auditability**: Immutable append-only event stream for compliance-critical events.

---

## 8. Reference Architecture (v1)

1. **Generation Orchestrator (Laravel service layer)**
   - Creates run context and correlation ID.
   - Calls provider adapter.

2. **Guardrail Engine**
   - Executes ordered checks.
   - Returns structured decision object.

3. **Event Logger / Telemetry Pipeline**
   - Emits structured events (`generation.started`, `generation.completed`, `guardrail.failed`, etc.).
   - Writes to primary operational store + analytics sink.

4. **Operational Data Store**
   - Query-optimized tables/collections for debugging and review tooling.

5. **Analytics Warehouse Sink**
   - Denormalized fact tables for dashboards and offline analysis.

6. **Review Queue Service**
   - Holds flagged artifacts and moderator outcomes.

7. **Alerting Layer**
   - Rule-based alerts from metrics and anomaly thresholds.

---

## 9. Data Model (Minimum)

### 9.1 `ai_generation_runs`

- `id` (UUID)
- `correlation_id`
- `feature_name` (e.g., `learning_pack_generation`)
- `actor_type` (`child`, `parent`, `system`)
- `actor_ref`
- `child_profile_id` (nullable)
- `document_id` (nullable)
- `provider`
- `model_name`
- `model_version`
- `prompt_template_version`
- `started_at`, `completed_at`
- `final_status` (`served`, `blocked`, `review_required`, `error`)
- `final_risk_score` (0–100)

### 9.2 `ai_generation_artifacts`

- `id` (UUID)
- `run_id`
- `artifact_type` (`raw_output`, `normalized_json`, `redacted_output`)
- `content_encrypted` (blob/text)
- `content_hash`
- `schema_name` / `schema_version` (nullable)
- `created_at`

### 9.3 `ai_guardrail_results`

- `id` (UUID)
- `run_id`
- `check_name`
- `check_version`
- `result` (`pass`, `warn`, `fail`)
- `risk_points`
- `reason_codes` (array)
- `details_json`
- `created_at`

### 9.4 `ai_review_cases`

- `id` (UUID)
- `run_id`
- `case_status` (`open`, `in_review`, `resolved`)
- `severity` (`low`, `medium`, `high`, `critical`)
- `assigned_to` (nullable)
- `resolution` (`approved`, `rejected`, `redacted`, `escalated`)
- `resolution_notes`
- `resolved_at`

### 9.5 `ai_review_audit_log`

- `id` (UUID)
- `case_id`
- `actor_id`
- `action`
- `from_state`
- `to_state`
- `metadata_json`
- `created_at`

---

## 10. Event Taxonomy

Mandatory events:

- `generation.started`
- `generation.provider_request_sent`
- `generation.provider_response_received`
- `generation.schema_validation_failed`
- `generation.guardrail_failed`
- `generation.review_case_created`
- `generation.completed`
- `generation.delivery_blocked`

All events must include:

- `event_id`, `event_name`, `occurred_at`
- `run_id`, `correlation_id`
- `environment` (`dev`, `staging`, `prod`)
- `service_version`

---

## 11. Guardrail Policy Framework

Each guardrail check must define:

- **Intent**: what risk this check controls.
- **Inputs**: which artifacts/metadata it reads.
- **Logic**: deterministic rules and/or model-based classifier details.
- **Thresholds**: pass/warn/fail boundaries.
- **Actions**: block, redact, send for review, allow.
- **Owner**: team/person accountable.
- **Review cadence**: e.g., monthly threshold tuning.

Initial checks to implement first:

1. Child safety and abuse-related content filter.
2. Hate/harassment/discrimination filter.
3. Sexual content filter (strictly age-gated).
4. Self-harm / dangerous advice filter.
5. Personal data leakage detector.
6. Curriculum factuality sanity checks (rule-based starter set).

---

## 12. Dashboards and Alerting

### 12.1 Core Dashboard Panels

- Total generations by feature (hour/day/week).
- p50/p95 latency end-to-end.
- Guardrail fail rate by check name.
- Blocked vs served rate by feature and model version.
- Review queue size and median time-to-resolution.
- Incident heatmap by severity.

### 12.2 Alert Rules (Initial)

- Guardrail fail rate > 3x trailing 7-day baseline for 15 minutes.
- Schema validation failures > 2% of runs in 10 minutes.
- Critical review cases opened > threshold per hour.
- p95 generation latency breaches SLA for 3 consecutive windows.

---

## 13. Compliance, Privacy, and Retention

- Minimize stored personal data in prompts/outputs where possible.
- Store sensitive artifacts encrypted with strict access controls.
- Support right-to-delete: when child/account data is deleted, linked artifacts must be hard-deleted or irreversibly anonymized according to policy.
- Default retention for raw AI artifacts: 90 days in hot store, then aggregated/anonymized records retained for long-term analytics.
- Access to raw artifacts must be role-based and fully audited.

---

## 14. Rollout Plan

### Phase 1 (Foundation)

- Run-level logging + correlation IDs.
- Schema validation logging.
- Basic safety moderation + fail-close policy.
- Minimal dashboards and alerts.

### Phase 2 (Governance)

- Full guardrail framework + policy versioning.
- Human review queue and audit log.
- Risk scoring and tiered workflows.

### Phase 3 (Optimization)

- Automated trend analysis and drift detection.
- Experimentation support (prompt/model version comparison).
- Feedback loop tooling for prompt and model quality improvements.

---

## 15. Acceptance Criteria

- 100% of AI generation requests produce a persisted run record.
- 100% of served outputs have guardrail result records.
- Blocked outputs are never returned to child-facing endpoints.
- On-call can identify run history and failure reason within 5 minutes using dashboards/logs.
- At least one weekly governance report can be generated from stored telemetry without manual data reconstruction.

---

## 16. Open Questions

- Which moderation provider(s) and fallback strategy should be used?
- What exact legal retention periods are required by operating region?
- Should factuality checks be centralized or per-subject (math/science/language) from the start?
- What reviewer staffing model and SLA are realistic for launch?

---

## 17. Alignment with Existing Learny Specs

- Supports the app’s ethical AI positioning in `specs/business_specs.md`.
- Extends the “observability and logging” non-functional requirement in `specs/technical_specs.md` with implementation-level requirements.
